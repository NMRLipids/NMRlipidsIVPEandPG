{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOI=\"10.5281/zenodo.3266240\"\n",
    "\n",
    "#UNITEDATOM stands for the name of the lipid in dic_lipids.py dictionary used bu buildH_calcOP.py\n",
    "\n",
    "user_information = \"\"\"\n",
    "POPCPOPG 7:3 310K gromos-ckp\n",
    "#NMRLIPIDS BEGIN\n",
    "\n",
    "@SIM\n",
    "@MAPPING=POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt\n",
    "@SYSTEM=POPCPOPG_7:3_T310K\n",
    "@SOFTWARE=gromacs\n",
    "@FF=GROMOS-CKP\n",
    "@FF_SOURCE=??\n",
    "@FF_DATE=?/?/????\n",
    "@TRJ=total_4_500.xtc\n",
    "@TPR=md_0.tpr\n",
    "@PREEQTIME=0\n",
    "@TIMELEFTOUT=400\n",
    "@UNITEDATOM=POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG\n",
    "\n",
    "\n",
    "@POPC=POPC\n",
    "@POPG=LPOG\n",
    "@POPS=POPS\n",
    "@POPE=POPE\n",
    "\n",
    "@POT=K\n",
    "@SOD=NA\n",
    "@CLA=CL\n",
    "@CAL=CA\n",
    "@SOL=SOL\n",
    "\n",
    "@NPOPC=[0,0]\n",
    "@NPOPG=[0,0]\n",
    "@NPOPS=[0,0]\n",
    "@NPOPE=[0,0]\n",
    "\n",
    "@NPOT=0\n",
    "@NSOD=0\n",
    "@NCLA=0\n",
    "@NCAL=0\n",
    "@NSOL=0\n",
    "\n",
    "@TEMPERATURE=0\n",
    "@TRJLENGTH=0\n",
    "\n",
    "\n",
    "#NMRLIPIDS END\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Working directory\n",
    "dir_wrk  = \"/media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with files and directories\n",
    "import os\n",
    "\n",
    "#For quering webs\n",
    "import urllib.request\n",
    "from urllib.error import URLError,HTTPError\n",
    "\n",
    "# From time monitoring\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Python program to find SHA256 hash string of a file\n",
    "import hashlib\n",
    "\n",
    "# For dealing with excel and cvs \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "#To make real independent copies of lists\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK\n"
     ]
    }
   ],
   "source": [
    "dir_wrk = os.path.normpath(dir_wrk)\n",
    "#mapping_dir =os.path.normpath(mapping_dir)\n",
    "print(dir_wrk)\n",
    "#print(mapping_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the DOI link is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.5281/zenodo.3266240\n",
      "Status of the DOI link: OK\n"
     ]
    }
   ],
   "source": [
    "DOI_url = 'https://doi.org/' + DOI\n",
    "print(DOI_url)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(DOI_url)\n",
    "    print(\"Status of the DOI link: {0}\".format(response.msg))\n",
    "except HTTPError as e:\n",
    "    print(DOI_url)\n",
    "    print('The server couldn\\'t fulfill the request.')\n",
    "    print('Error code: ', e.code)\n",
    "    user_information = \"\"\n",
    "    print('The code will not proceed, please fix DOI')\n",
    "except URLError as e:\n",
    "    print(DOI_url)\n",
    "    print('We failed to reach a server.')\n",
    "    print('Reason: ', e.reason)\n",
    "    user_information = \"\"\n",
    "    print('The code will not proceed, please fix DOI')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'ID': 0, 'DOI': '10.5281/zenodo.3266240', 'MAPPING': 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt', 'SYSTEM': 'POPCPOPG_7:3_T310K', 'SOFTWARE': 'gromacs', 'FF': 'GROMOS-CKP', 'FF_SOURCE': '??', 'FF_DATE': '?/?/????', 'TRJ': 'total_4_500.xtc', 'TPR': 'md_0.tpr', 'PREEQTIME': '0', 'TIMELEFTOUT': '400', 'UNITEDATOM': 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG', 'POPC': 'POPC', 'POPG': 'LPOG', 'POPS': 'POPS', 'POPE': 'POPE', 'POT': 'K', 'SOD': 'NA', 'CLA': 'CL', 'CAL': 'CA', 'SOL': 'SOL', 'NPOPC': '[0,0]', 'NPOPG': '[0,0]', 'NPOPS': '[0,0]', 'NPOPE': '[0,0]', 'NPOT': '0', 'NSOD': '0', 'NCLA': '0', 'NCAL': '0', 'NSOL': '0', 'TEMPERATURE': '0', 'TRJLENGTH': '0'}]\n"
     ]
    }
   ],
   "source": [
    "bNMRLIPIDS = False #Check if the link contains NMRLIPIDS metadata\n",
    "nsims =0 # Counter number of simulations in a submission\n",
    "sims = [] #Array with the dictionary containing the information of a simulation\n",
    "\n",
    "for line in user_information.split(\"\\n\"):\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    if \"#NMRLIPIDS BEGIN\" in line:\n",
    "        bNMRLIPIDS = True\n",
    "        continue\n",
    "    if \"#NMRLIPIDS END\" in line:\n",
    "        bNMRLIPIDS = False\n",
    "        continue\n",
    "    if \"@SIM\" in line:\n",
    "        #sims.append({\"ID\" : nsims, \"STATUS\" : 0})\n",
    "        sims.append({\"ID\" : nsims})\n",
    "        nsims += 1\n",
    "        sims[-1][\"DOI\"] = DOI\n",
    "        continue\n",
    "    if not bNMRLIPIDS:\n",
    "        continue\n",
    "    if line.strip()[0] == \"@\":\n",
    "        key, value = line.split(\"=\")\n",
    "        sims[-1][key.strip('@')] = value\n",
    "print(nsims)\n",
    "print(sims)      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['GROMACS', 'AMBER', 'NAMD', 'CHARMM', 'OPENMM'])\n"
     ]
    }
   ],
   "source": [
    "#Molecules dictionary\n",
    "\n",
    "lipids_dict = {\n",
    "                'POPC' : {\"REQUIRED\": False,\n",
    "                             \"TYPE\": \"string\",\n",
    "                         },\n",
    "                'POPG' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                'POPS' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                'POPE' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                }\n",
    "\n",
    "molecules_dict = {\n",
    "                \n",
    "                'POT' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                'SOD' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                'CLA' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                'CAL' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                'SOL' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                        },\n",
    "                }\n",
    "\n",
    "\n",
    "               \n",
    "molecule_numbers_dict = {\n",
    "                'NPOPC' : {\"REQUIRED\": False,\n",
    "                              \"TYPE\": \"array\",\n",
    "                          },\n",
    "                'NPOPG' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"array\",\n",
    "                         },\n",
    "                'NPOPS' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"array\",\n",
    "                        },\n",
    "                'NPOPE' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"array\",\n",
    "                        },\n",
    "    \n",
    "               'NPOT' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"integer\",\n",
    "                        },\n",
    "                'NSOD' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"integer\",\n",
    "                        },\n",
    "                'NCLA' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"integer\",\n",
    "                        },\n",
    "                'NCAL' : {\"REQUIRED\": False,\n",
    "                             \"TYPE\" : \"integer\",\n",
    "                         },\n",
    "                'NSOL' : {\"REQIRED\": False,\n",
    "                            \"TYPE\" : \"integer\",\n",
    "                        },\n",
    "    \n",
    "                }\n",
    "               \n",
    "                \n",
    "                \n",
    "\n",
    "# Gromacs\n",
    "gromacs_dict = {\n",
    "               'INI' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"files\",\n",
    "                        \"EXTENSION\" : (\"gro\", \"pdb\",),\n",
    "                       }, # Could be not needed in the future (tpr)\n",
    "               'MDP' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"mdp\",),\n",
    "                       }, # Could be not needed in the future (tpr)\n",
    "               'TRJ' : {\"REQUIRED\": True,\n",
    "                        \"TYPE\" : \"files\",\n",
    "                        \"EXTENSION\" : (\"xtc\",\"trr\",),\n",
    "                       },\n",
    "               'TPR' : {\"REQUIRED\": True,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"tpr\",),\n",
    "                       },\n",
    "               'CPT' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"cpt\",),\n",
    "                       },\n",
    "               'TOP' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"file\",\n",
    "                        \"EXTENSION\" : (\"top\",),\n",
    "                       },\n",
    "               'ITP' : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"files\",\n",
    "                        \"EXTENSION\" : (\"itp\",),\n",
    "                       },\n",
    "               'FF'  : {\"REQUIRED\": False,\n",
    "                        \"TYPE\" : \"string\",\n",
    "                       },\n",
    "               'FF_SOURCE' : {\"REQUIRED\": False,\n",
    "                              \"TYPE\" : \"string\",\n",
    "                              },\n",
    "               'FF_DATE' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"date\",\n",
    "                           },\n",
    "               'DOI' : {\"REQUIRED\": True,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                           },\n",
    "    \n",
    "               'SYSTEM' : {\"REQUIRED\": True,\n",
    "                            \"TYPE\" : \"string\",\n",
    "                           },\n",
    "            'TEMPERATURE' : {\"REQUIRED\": False,\n",
    "                            \"TYPE\" : \"integer\",\n",
    "                            },\n",
    "             'TRJLENGTH' : {\"REQUIRED\": False,\n",
    "                           \"TYPE\" : \"integer\",\n",
    "                           },\n",
    "            'PREEQTIME' : {\"REQUIRED\": False,\n",
    "                          \"TYPE\" : \"integer\",\n",
    "                          },\n",
    "         'TIMELEFTOUT' : {\"REQUIRED\": False,\n",
    "                          \"TYPE\" : \"integer\",\n",
    "                         },\n",
    "            'MAPPING' : {\"REQUIRED\": True,\n",
    "                             \"TYPE\" : \"string\",\n",
    " #                        \"EXTENSION\": (\"txt\"),\n",
    "                             },\n",
    "          'UNITEDATOM' : {\"REQUIRED\": False,\n",
    "                          \"TYPE\" : \"string\",\n",
    "                         },\n",
    "              \n",
    "               }\n",
    "\n",
    "# Amber\n",
    "amber_dict = {}\n",
    "\n",
    "# NAMD\n",
    "namd_dict = {   \n",
    "            'TRJ' : { \"REQUIRED\": True,\n",
    "                      \"TYPE\": \"files\",\n",
    "                      \"EXTENSION\": (\"dcd\"),\n",
    "                    },\n",
    "            'INP' : { \"REQUIRED\": False,\n",
    "                      \"TYPE\": \"file\",\n",
    "                      \"EXTENSION\": (\".inp\"),\n",
    "                    },\n",
    "            'LOG' : { \"REQUIRED\": False,\n",
    "                      \"TYPE\": \"files\",\n",
    "                      \"EXTENSION\": (\"log\"),\n",
    "                      # can be parsed to get software version etc.\n",
    "                    },\n",
    "            'PSF' : { \"REQUIRED\": False,\n",
    "                      \"TYPE\": \"file\",\n",
    "                      \"EXTENSION\": (\"psf\"),\n",
    "                    },\n",
    "            'FF'  :  { \"REQUIRED\": False,\n",
    "                      \"TYPE\" : \"string\",\n",
    "                    },\n",
    "            'FF_SOURCE' : {\"REQUIRED\": False,\n",
    "                           \"TYPE\" : \"string\",\n",
    "                              },\n",
    "            'FF_DATE' : {\"REQUIRED\": False,\n",
    "                         \"TYPE\" : \"date\",\n",
    "                        },\n",
    "            'PDB'  : { \"REQUIRED\": True,\n",
    "                    \"TYPE\": \"file\",\n",
    "                    \"EXTENSION\": \"pdb\",}\n",
    "               }\n",
    "          \n",
    "# CHARMM\n",
    "charmm_dict = {}\n",
    "\n",
    "# OPENMM\n",
    "openmm_dict = {}\n",
    "\n",
    "# SOFTWARE\n",
    "software_dict = {\n",
    "                \"GROMACS\" : gromacs_dict, \n",
    "                \"AMBER\"   : amber_dict,\n",
    "                \"NAMD\"    : namd_dict,\n",
    "                \"CHARMM\"  : charmm_dict,\n",
    "                \"OPENMM\"  : openmm_dict,\n",
    "                }\n",
    "\n",
    "print(software_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check software used by the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 0 uses supported software GROMACS and will be further processed\n"
     ]
    }
   ],
   "source": [
    "sims_valid_software = []\n",
    "for sim in sims:\n",
    "    if sim['SOFTWARE'].upper() in software_dict.keys():\n",
    "        msg_info = \"Simulation {0} uses supported software {1} and will be further processed\"\n",
    "        print (msg_info.format(sim['ID'], sim['SOFTWARE'].upper()))\n",
    "        sims_valid_software.append(sim.copy())\n",
    "    else:\n",
    "        msg_err=\"Simulation {0} performed in an UNSUPPORTED software {1} and will NOT be further processed\"\n",
    "        print(msg_err.format(sim[\"ID\"], sim[\"SOFTWARE\"].upper()))\n",
    "#print(sims_valid_software) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that all entry keys provided for each simulation are valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('ID', 0), ('DOI', '10.5281/zenodo.3266240'), ('MAPPING', 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt'), ('SYSTEM', 'POPCPOPG_7:3_T310K'), ('SOFTWARE', 'gromacs'), ('FF', 'GROMOS-CKP'), ('FF_SOURCE', '??'), ('FF_DATE', '?/?/????'), ('TRJ', 'total_4_500.xtc'), ('TPR', 'md_0.tpr'), ('PREEQTIME', '0'), ('TIMELEFTOUT', '400'), ('UNITEDATOM', 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG'), ('POPC', 'POPC'), ('POPG', 'LPOG'), ('POPS', 'POPS'), ('POPE', 'POPE'), ('POT', 'K'), ('SOD', 'NA'), ('CLA', 'CL'), ('CAL', 'CA'), ('SOL', 'SOL'), ('NPOPC', '[0,0]'), ('NPOPG', '[0,0]'), ('NPOPS', '[0,0]'), ('NPOPE', '[0,0]'), ('NPOT', '0'), ('NSOD', '0'), ('NCLA', '0'), ('NCAL', '0'), ('NSOL', '0'), ('TEMPERATURE', '0'), ('TRJLENGTH', '0')])\n",
      "All entries in simulation 0 are understood and will be further processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims_valid_entries = []\n",
    "for sim in sims_valid_software:\n",
    "    #print(\"ID {0}\".format(sim[\"ID\"]))\n",
    "    wrong_key_entries = 0\n",
    "    software_dict_name = \"{0}_dict\".format(sim['SOFTWARE'].lower())\n",
    "    print(sim.items())\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(key_sim, value_sim)\n",
    "        #print(key_sim.upper())\n",
    "        if key_sim.upper() in (\"ID\", \"SOFTWARE\"):\n",
    "            #print(\"NOT REQUIRED\")\n",
    "            continue\n",
    "    #Anne: check if key is in molecules_dict or molecule_numbers_dict too\n",
    "        if key_sim.upper() not in software_dict[sim['SOFTWARE'].upper()].keys() and key_sim.upper() not in molecules_dict.keys() and key_sim.upper() not in lipids_dict.keys() and key_sim.upper() not in molecule_numbers_dict:\n",
    "            print (\"{0} NOT in {1}\".format(key_sim, software_dict_name))\n",
    "            wrong_key_entries += 1\n",
    "    if wrong_key_entries:\n",
    "        print(\"Simulation {0} has {1} unknown entry/ies and won't be longer considered, please correct.\\n\".format(sim['ID'],wrong_key_entries))\n",
    "    else:\n",
    "        msg_info = \"All entries in simulation {0} are understood and will be further processed\\n\"\n",
    "        print (msg_info.format(sim['ID']))\n",
    "        sims_valid_entries.append(sim.copy())\n",
    "#print(sims_valid_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process entries with files information to contain file names in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "total_4_500.xtc added to list\n",
      "md_0.tpr added to list\n"
     ]
    }
   ],
   "source": [
    "sims_files_to_array = deepcopy(sims_valid_entries)\n",
    "\n",
    "for sim in sims_files_to_array:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            if \"file\" in entry_type:\n",
    "                if isinstance(value_sim, list): continue  \n",
    "                files_list = []\n",
    "                print(\"{0} added to list\".format(value_sim))\n",
    "                # Place filenames into arrays\n",
    "                for file_provided in value_sim.split(\";\"):\n",
    "                    files_list.append([file_provided.strip()])\n",
    "                sim[key_sim] = files_list\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "#print(sims_files_to_array)\n",
    "#print(sims_valid_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for multiple files in entries that can only contain one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n"
     ]
    }
   ],
   "source": [
    "sims_valid_file_entries = []\n",
    "for sim in sims_files_to_array:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    files_issues = 0\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            if entry_type == \"file\"  and len(value_sim) > 1:\n",
    "                print(\"Multiple values found in {0} and only one allowed (Please correct):\\n {1}\".format(key_sim,value_sim))\n",
    "                files_issues += 1\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "    if files_issues:\n",
    "        print(\"Sim {0} will be no longer processed\".format(sim[\"ID\"]))\n",
    "    else:\n",
    "        sims_valid_file_entries.append(sim.copy())\n",
    "#print(sims_valid_file_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the submitted simulation has rssion has all required files and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "All required entries present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims_required_entries = []\n",
    "for sim in sims_valid_file_entries:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]))\n",
    "    missing_required_keys = 0\n",
    "    for key, value in software_dict[sim['SOFTWARE'].upper()].items():\n",
    "        if value[\"REQUIRED\"]:\n",
    "            try:\n",
    "                sim[key]\n",
    "            except:\n",
    "                print(\"Entry not found: {0} {1}\".format(key, value))\n",
    "                missing_required_keys += 1\n",
    "    if missing_required_keys:\n",
    "        print(\"{0} missing required entry/ies, please correct.\".format(missing_required_keys))\n",
    "        print(\"Entry with ID={0} will not be further processed.\\n\".format(sim[\"ID\"]))\n",
    "    else:\n",
    "        print(\"All required entries present.\\n\")\n",
    "        sims_required_entries.append(sim.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check status links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download link\n",
    "def download_link(doi, file):\n",
    "    if \"zenodo\" in doi.lower():\n",
    "        zenodo_entry_number = doi.split(\".\")[2]\n",
    "        return 'https://zenodo.org/record/' + zenodo_entry_number + '/files/' + file\n",
    "    else:\n",
    "        print (\"DOI provided: {0}\".format(doi))\n",
    "        print (\"Repository not validated. Please upload the data for example to zenodo.org\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "All links work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(sims_required_entries)\n",
    "sims_working_links = []         \n",
    "for sim in sims_required_entries:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]))\n",
    "    wrong_links = 0\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(\"key_sim = {0} => value_sim = {1}\".format(key_sim, value_sim))\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            extension_type = software_sim[key_sim]['EXTENSION']\n",
    "            #print(\"entry_type = {0}\".format(entry_type))\n",
    "            if \"file\" in entry_type and \"txt\" not in extension_type:\n",
    "                for file_provided in value_sim:\n",
    "                    #print(\"File={0}\".format(file_provided[0]))\n",
    "                    file_url = download_link(DOI, file_provided[0])\n",
    "                    if file_url == \"\":\n",
    "                        \n",
    "                        wrong_links += 1\n",
    "                        continue\n",
    "                    try:\n",
    "                        if key_sim == 'INI':\n",
    "                            continue\n",
    "                        response = urllib.request.urlopen(file_url)\n",
    "                        #print(\"Status of the DOI link: {0}\".format(response.msg))\n",
    "                    except HTTPError as e:\n",
    "                        print(\"\\nkey={0} => file={1}\".format(key_sim, file_provided[0]))\n",
    "                        print(file_url)\n",
    "                        print('The server couldn\\'t fulfill the request.')\n",
    "                        print('Error code: ', e.code)\n",
    "                        wrong_links += 1\n",
    "                    except URLError as e:\n",
    "                        print(key_sim, file_provided[0])\n",
    "                        print(file_url)\n",
    "                        print('We failed to reach a server.')\n",
    "                        print('Reason: ', e.reason)\n",
    "                        wrong_links += 1\n",
    "                    else:\n",
    "                        pass\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "    if wrong_links:\n",
    "        print(\"{0} link/s failed, please correct.\".format(wrong_links))\n",
    "        print(\"Sim={0} will not be further processed.\\n\".format(sim[\"ID\"]))\n",
    "    else:\n",
    "        print(\"All links work.\\n\")\n",
    "        sims_working_links.append(sim.copy())\n",
    "#print(sims_working_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRJ: 100%|██████████| 1/1 [00:00<00:00, 2955.82it/s]\n",
      "TPR: 100%|██████████| 1/1 [00:00<00:00, 2172.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create temporary directory where to download files and analyze them\n",
    "dir_tmp = os.path.join(dir_wrk, \"tmp/\")\n",
    "if (not os.path.isdir(dir_tmp)): os.mkdir(dir_tmp)\n",
    "\n",
    "for sim in sims_working_links:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    dir_sim = os.path.join(dir_tmp, str(sim[\"ID\"]))\n",
    "    if (not os.path.isdir(dir_sim)): os.mkdir(dir_sim)\n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(\"key_sim = {0} => value_sim = {1}\".format(key_sim, value_sim))\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            extension_type = software_sim[key_sim]['EXTENSION']\n",
    "            #print(\"entry_type = {0}\".format(entry_type))\n",
    "            if \"file\" in entry_type and \"txt\" not in extension_type:\n",
    "                for file_provided in tqdm(value_sim, desc = key_sim):\n",
    "                    file_url = download_link(DOI, file_provided[0])\n",
    "                    file_name = os.path.join(dir_sim, file_provided[0])\n",
    "                    if (not os.path.isfile(file_name)):\n",
    "                        response = urllib.request.urlretrieve(file_url, file_name)\n",
    "        except: #It is normal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate hash downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0\n",
      "              NAME TYPE REQUIRED                                      HASH\n",
      "0  total_4_500.xtc  TRJ     True  3d0ca721c24d7f22d09178f10f1dd89a333dfe07\n",
      "1         md_0.tpr  TPR     True  35dd6bcb5bf03400b81c070292e36025c48dc1a6\n",
      "\n",
      "['3d0ca721c24d7f22d09178f10f1dd89a333dfe07', '35dd6bcb5bf03400b81c070292e36025c48dc1a6']\n",
      "\n",
      "[{'ID': 0, 'DOI': '10.5281/zenodo.3266240', 'MAPPING': 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt', 'SYSTEM': 'POPCPOPG_7:3_T310K', 'SOFTWARE': 'gromacs', 'FF': 'GROMOS-CKP', 'FF_SOURCE': '??', 'FF_DATE': '?/?/????', 'TRJ': [['total_4_500.xtc', '3d0ca721c24d7f22d09178f10f1dd89a333dfe07']], 'TPR': [['md_0.tpr', '35dd6bcb5bf03400b81c070292e36025c48dc1a6']], 'PREEQTIME': '0', 'TIMELEFTOUT': '400', 'UNITEDATOM': 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG', 'POPC': 'POPC', 'POPG': 'LPOG', 'POPS': 'POPS', 'POPE': 'POPE', 'POT': 'K', 'SOD': 'NA', 'CLA': 'CL', 'CAL': 'CA', 'SOL': 'SOL', 'NPOPC': '[0,0]', 'NPOPG': '[0,0]', 'NPOPS': '[0,0]', 'NPOPE': '[0,0]', 'NPOT': '0', 'NSOD': '0', 'NCLA': '0', 'NCAL': '0', 'NSOL': '0', 'TEMPERATURE': '0', 'TRJLENGTH': '0'}]\n"
     ]
    }
   ],
   "source": [
    "dir_tmp = os.path.join(dir_wrk, \"tmp/\")\n",
    "sims_hashes = deepcopy(sims_working_links)\n",
    "\n",
    "for sim in sims_hashes:\n",
    "    print(\"ID {0}\".format(sim[\"ID\"]), flush=True)\n",
    "    software_sim = software_dict[sim['SOFTWARE'].upper()]\n",
    "    dir_sim = os.path.join(dir_tmp, str(sim[\"ID\"]))\n",
    "    \n",
    "    #list_containing the sha1 sums for all required files\n",
    "    sha1_list_requied = []\n",
    "    \n",
    "    # Make empty dataframe with the desired columns\n",
    "    df_files = pd.DataFrame(columns=['NAME','TYPE','REQUIRED','HASH'])\n",
    "    \n",
    "    for key_sim, value_sim in sim.items():\n",
    "        #print(\"key_sim = {0} => value_sim = {1}\".format(key_sim, value_sim))\n",
    "        try:\n",
    "            entry_type = software_sim[key_sim]['TYPE']\n",
    "            #print(\"entry_type = {0}\".format(entry_type))\n",
    "            if \"file\" in entry_type:\n",
    "                files_list = []\n",
    "                for file_provided in value_sim:\n",
    "                    file_name = os.path.join(dir_sim, file_provided[0])\n",
    "                    sha1_hash = hashlib.sha1()\n",
    "                    with open(file_name,\"rb\") as f:\n",
    "                        # Read and update hash string value in blocks of 4K\n",
    "                        for byte_block in iter(lambda: f.read(4096),b\"\"):\n",
    "                            sha1_hash.update(byte_block)\n",
    "                        #print(file_provided, sha256_hash.hexdigest())\n",
    "                        df_files = df_files.append({\n",
    "                            \"NAME\":file_provided[0],\n",
    "                            \"TYPE\":key_sim,\n",
    "                            \"REQUIRED\": software_dict[sim['SOFTWARE'].upper()][key_sim]['REQUIRED'],\n",
    "                            \"HASH\":sha1_hash.hexdigest(),\n",
    "                        }, ignore_index=True)\n",
    "                    files_list.append([file_provided[0], sha1_hash.hexdigest()])\n",
    "                #Find the keys of the required files to calculate the master_hash \n",
    "                if software_dict[sim['SOFTWARE'].upper()][key_sim]['REQUIRED'] == True:\n",
    "                    sha1_list_requied.append(sha1_hash.hexdigest())\n",
    "                sim[key_sim] = files_list #Problematic\n",
    "        except: #It is notmal that fails for \"ID\" and \"SOFTWARE\"\n",
    "            continue\n",
    "    print(df_files)\n",
    "    print(\"\\n{0}\\n\".format(sha1_list_requied))      \n",
    "    # Calculate the hash of a file contaning the hashes of each of the required files\n",
    "    # This should be always invariant as it will be used unique identifier for a simualtion\n",
    "    # Note order the hashes of the required files before calculating the hash (That means that the required files cannot change)\n",
    "print(sims_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'ID': 0, 'DOI': '10.5281/zenodo.3266240', 'MAPPING': 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt', 'SYSTEM': 'POPCPOPG_7:3_T310K', 'SOFTWARE': 'gromacs', 'FF': 'GROMOS-CKP', 'FF_SOURCE': '??', 'FF_DATE': '?/?/????', 'TRJ': [['total_4_500.xtc']], 'TPR': [['md_0.tpr']], 'PREEQTIME': '0', 'TIMELEFTOUT': '400', 'UNITEDATOM': 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG', 'POPC': 'POPC', 'POPG': 'LPOG', 'POPS': 'POPS', 'POPE': 'POPE', 'POT': 'K', 'SOD': 'NA', 'CLA': 'CL', 'CAL': 'CA', 'SOL': 'SOL', 'NPOPC': '[0,0]', 'NPOPG': '[0,0]', 'NPOPS': '[0,0]', 'NPOPE': '[0,0]', 'NPOT': '0', 'NSOD': '0', 'NCLA': '0', 'NCAL': '0', 'NSOL': '0', 'TEMPERATURE': '0', 'TRJLENGTH': '0'}]\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sims_working_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dictionary of mapping files and save it in the simulation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "POPC\n",
      "1\n",
      "POPG\n",
      "{'POPC': 'mappingPOPCgromos-ckp.txt', 'POPG': 'mappingPOPGgromos-ckp.txt'}\n",
      "{'ID': 0, 'DOI': '10.5281/zenodo.3266240', 'MAPPING': 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt', 'SYSTEM': 'POPCPOPG_7:3_T310K', 'SOFTWARE': 'gromacs', 'FF': 'GROMOS-CKP', 'FF_SOURCE': '??', 'FF_DATE': '?/?/????', 'TRJ': [['total_4_500.xtc']], 'TPR': [['md_0.tpr']], 'PREEQTIME': '0', 'TIMELEFTOUT': '400', 'UNITEDATOM': 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG', 'POPC': 'POPC', 'POPG': 'LPOG', 'POPS': 'POPS', 'POPE': 'POPE', 'POT': 'K', 'SOD': 'NA', 'CLA': 'CL', 'CAL': 'CA', 'SOL': 'SOL', 'NPOPC': '[0,0]', 'NPOPG': '[0,0]', 'NPOPS': '[0,0]', 'NPOPE': '[0,0]', 'NPOT': '0', 'NSOD': '0', 'NCLA': '0', 'NCAL': '0', 'NSOL': '0', 'TEMPERATURE': '0', 'TRJLENGTH': '0', 'MAPPING_DICT': {'POPC': 'mappingPOPCgromos-ckp.txt', 'POPG': 'mappingPOPGgromos-ckp.txt'}, 'UADICTIONARY': {'POPC': 'GROMOS_CKP_POPC', 'POPG': 'GROMOS_CKP_POPG'}}\n"
     ]
    }
   ],
   "source": [
    "for sim in sims_working_links :\n",
    "    mapping = sim['MAPPING'].split(',')\n",
    "    mapping_dict = {}\n",
    "\n",
    "    for i in range(0, int(len(mapping)/2)):\n",
    "        lipid = mapping[2*i]\n",
    "        print(i)\n",
    "        print(lipid)\n",
    "        path = mapping[2*i+1]\n",
    "        mapping_dict[lipid]=path\n",
    "\n",
    "#print(mapping_dict)\n",
    "\n",
    "    sim['MAPPING_DICT'] = mapping_dict\n",
    "\n",
    "    print(sim['MAPPING_DICT'])\n",
    "\n",
    "#in case of a united atom simulation make a dictionary of united atom names \n",
    "    if sim.get('UNITEDATOM'):\n",
    "        unitedAtoms = sim['UNITEDATOM'].split(',')\n",
    "        unitedAtomsDic = {}\n",
    "        for i in range(0, int(len(unitedAtoms)/2)):\n",
    "            lipid = unitedAtoms[2*i]\n",
    "            UAlipid = unitedAtoms[2*i+1]\n",
    "            unitedAtomsDic[lipid]=UAlipid\n",
    "        sim['UADICTIONARY'] = unitedAtomsDic\n",
    "        \n",
    "    print(sim)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read molecule numbers into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POPC': 'mappingPOPCgromos-ckp.txt', 'POPG': 'mappingPOPGgromos-ckp.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(sim['MAPPING_DICT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/conf.gro\n",
      "                      :-) GROMACS - gmx trjconv, 2020 (-:\n",
      "\n",
      "                            GROMACS is written by:\n",
      "     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen\n",
      "    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    \n",
      " Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     \n",
      "  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  \n",
      "  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis\n",
      "    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    \n",
      "  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   \n",
      "    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   \n",
      "    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  \n",
      "   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen \n",
      " Christian Wennberg    Maarten Wolf      Artem Zhmurov   \n",
      "                           and the project leaders:\n",
      "        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel\n",
      "\n",
      "Copyright (c) 1991-2000, University of Groningen, The Netherlands.\n",
      "Copyright (c) 2001-2019, The GROMACS development team at\n",
      "Uppsala University, Stockholm University and\n",
      "the Royal Institute of Technology, Sweden.\n",
      "check out http://www.gromacs.org for more information.\n",
      "\n",
      "GROMACS is free software; you can redistribute it and/or modify it\n",
      "under the terms of the GNU Lesser General Public License\n",
      "as published by the Free Software Foundation; either version 2.1\n",
      "of the License, or (at your option) any later version.\n",
      "\n",
      "GROMACS:      gmx trjconv, version 2020\n",
      "Executable:   /usr/local/gromacs/bin/gmx\n",
      "Data prefix:  /usr/local/gromacs\n",
      "Working dir:  /media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts\n",
      "Command line:\n",
      "  gmx trjconv -f /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/total_4_500.xtc -s /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/md_0.tpr -dump 0 -o /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/conf.gro\n",
      "\n",
      "Note that major changes are planned in future for trjconv, to improve usability and utility.\n",
      "Will write gro: Coordinate file in Gromos-87 format\n",
      "Reading file /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/md_0.tpr, VERSION 2018.3 (single precision)\n",
      "Reading file /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/md_0.tpr, VERSION 2018.3 (single precision)\n",
      "Select group for output\n",
      "Group     0 (         System) has 101000 elements\n",
      "Group     1 (          Other) has 26000 elements\n",
      "Group     2 (           POPC) has 26000 elements\n",
      "Group     3 (          Water) has 75000 elements\n",
      "Group     4 (            SOL) has 75000 elements\n",
      "Group     5 (      non-Water) has 26000 elements\n",
      "Select a group: Selected 0: 'System'\n",
      "Reading frame       0 time 400000.000   \n",
      "Precision of /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/total_4_500.xtc is 0.001 (nm)\n",
      "Reading frame       0 time 400000.000   \n",
      "Back Off! I just backed up /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/conf.gro to /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/#conf.gro.2#\n",
      "\n",
      "Dumping frame at t= 400000 ps\n",
      "Reading frame       1 time 400010.000   \n",
      "\n",
      "GROMACS reminds you: \"He's using code that only you and I know\" (Kate Bush)\n",
      "\n",
      "POPC\n",
      "resname POPC\n",
      "molecules\n",
      "<AtomGroup [<Atom 1: C1 of type C of resname POPC, resid 1 and segid SYSTEM>, <Atom 2: C2 of type C of resname POPC, resid 1 and segid SYSTEM>, <Atom 3: C3 of type C of resname POPC, resid 1 and segid SYSTEM>, ..., <Atom 25998: C48 of type C of resname POPC, resid 500 and segid SYSTEM>, <Atom 25999: C49 of type C of resname POPC, resid 500 and segid SYSTEM>, <Atom 26000: C50 of type C of resname POPC, resid 500 and segid SYSTEM>]>\n",
      "[<AtomGroup with 26000 atoms>]\n",
      "POPG\n",
      "resname LPOG\n",
      "molecules\n",
      "<AtomGroup []>\n",
      "POPS\n",
      "molecules\n",
      "<AtomGroup []>\n",
      "POPE\n",
      "molecules\n",
      "<AtomGroup []>\n",
      "membrane\n",
      "<AtomGroup [<Atom 1: C1 of type C of resname POPC, resid 1 and segid SYSTEM>, <Atom 2: C2 of type C of resname POPC, resid 1 and segid SYSTEM>, <Atom 3: C3 of type C of resname POPC, resid 1 and segid SYSTEM>, ..., <Atom 25998: C48 of type C of resname POPC, resid 500 and segid SYSTEM>, <Atom 25999: C49 of type C of resname POPC, resid 500 and segid SYSTEM>, <Atom 26000: C50 of type C of resname POPC, resid 500 and segid SYSTEM>]>\n",
      "43.961337202262186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akiirikk/anaconda3/lib/python3.7/site-packages/MDAnalysis/core/groups.py:2854: UserWarning: Empty string to select atoms, empty group returned.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ResidueGroup [<Residue POPC, 1>, <Residue POPC, 2>, <Residue POPC, 3>, ..., <Residue POPC, 498>, <Residue POPC, 499>, <Residue POPC, 500>]>\n",
      "<ResidueGroup []>\n",
      "<ResidueGroup []>\n",
      "<ResidueGroup []>\n",
      "K\n",
      "NA\n",
      "CL\n",
      "CA\n",
      "SOL\n",
      "{'ID': 0, 'DOI': '10.5281/zenodo.3266240', 'MAPPING': 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt', 'SYSTEM': 'POPCPOPG_7:3_T310K', 'SOFTWARE': 'gromacs', 'FF': 'GROMOS-CKP', 'FF_SOURCE': '??', 'FF_DATE': '?/?/????', 'TRJ': [['total_4_500.xtc']], 'TPR': [['md_0.tpr']], 'PREEQTIME': '0', 'TIMELEFTOUT': '400', 'UNITEDATOM': 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG', 'POPC': 'POPC', 'POPG': 'LPOG', 'POPS': 'POPS', 'POPE': 'POPE', 'POT': 'K', 'SOD': 'NA', 'CLA': 'CL', 'CAL': 'CA', 'SOL': 'SOL', 'NPOPC': [250, 250], 'NPOPG': [0, 0], 'NPOPS': [0, 0], 'NPOPE': [0, 0], 'NPOT': 0, 'NSOD': 0, 'NCLA': 0, 'NCAL': 0, 'NSOL': 25000, 'TEMPERATURE': '0', 'TRJLENGTH': '0', 'MAPPING_DICT': {'POPC': 'mappingPOPCgromos-ckp.txt', 'POPG': 'mappingPOPGgromos-ckp.txt'}, 'UADICTIONARY': {'POPC': 'GROMOS_CKP_POPC', 'POPG': 'GROMOS_CKP_POPG'}, 'GRO': '/media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/conf.gro'}\n"
     ]
    }
   ],
   "source": [
    "#Calculates numbers of lipid molecules in each leaflet. This is done by checking on which side of the centre \n",
    "#of mass the membrane each the centre of mass of a lipid molecule is.\n",
    "#If a lipid molecule is split so that headgroup and tails are their own residues, the centre of mass of the\n",
    "#headgroup is used in the calculation.\n",
    "################################################################################################################\n",
    "import MDAnalysis\n",
    "from MDAnalysis import Universe\n",
    "    \n",
    "for sim in sims_working_links :\n",
    "    ID = sim.get('ID')\n",
    "    tpr = str(dir_wrk)+ '/tmp/' + str(ID) + '/' + str(sim.get('TPR')).translate({ord(c): None for c in \"'][\"})\n",
    "    trj = str(dir_wrk)+ '/tmp/' + str(ID) + '/' + str(sim.get('TRJ')).translate({ord(c): None for c in \"'][\"})\n",
    "    gro = str(dir_wrk)+ '/tmp/' + str(ID) + '/conf.gro'\n",
    "    print(gro)\n",
    "    \n",
    " #   if str(sim.get('INI')).translate({ord(c): None for c in \"'][\"}) != '':\n",
    " #       gro = str(sim.get('INI')).translate({ord(c): None for c in \"'][\"})\n",
    " #       gro_path = str(dir_wrk) + '/tmp/' + str(ID) + '/' + gro\n",
    "\n",
    "  #  else:\n",
    "    !echo System | gmx trjconv -f {trj} -s {tpr} -dump 0 -o {gro}\n",
    "    \n",
    "  # add gro into dictionary for later use\n",
    "    sim['GRO'] = gro\n",
    "    \n",
    "    leaflet1 = 0 #total number of lipids in upper leaflet\n",
    "    leaflet2 = 0 #total number of lipids in lower leaflet\n",
    "    \n",
    "    u = Universe(gro)\n",
    "    lipids = []\n",
    "\n",
    "# select lipids \n",
    "    for key_mol in lipids_dict:\n",
    "        print(key_mol)\n",
    "        selection = \"\"\n",
    "        if key_mol in sim['MAPPING_DICT'].keys():\n",
    "            m_file = sim['MAPPING_DICT'][key_mol]\n",
    "            with open('./mapping_files/'+m_file,\"r\") as f:\n",
    "                    for line in f:\n",
    "                        if len(line.split()) > 2 and \"Individual atoms\" not in line:\n",
    "                            selection = selection + \"(resname \" + line.split()[2] + \" and name \" + line.split()[1] + \") or \"\n",
    "                        elif \"Individual atoms\" in line:\n",
    "                            continue\n",
    "                        else:\n",
    "                            selection = \"resname \" + sim.get(key_mol)\n",
    "                            print(selection)\n",
    "                            break\n",
    "        selection = selection.rstrip(' or ')\n",
    "        #print(\"selection    \" + selection)\n",
    "        molecules = u.select_atoms(selection)\n",
    "        print(\"molecules\")\n",
    "        print(molecules)\n",
    "        if molecules.n_residues > 0:\n",
    "            lipids.append(u.select_atoms(selection))\n",
    "            print(lipids) \n",
    "# join all the selected the lipids together to make a selection of the entire membrane and calculate the\n",
    "# z component of the centre of mass of the membrane\n",
    "    membrane = u.select_atoms(\"\")\n",
    "    R_membrane_z = 0\n",
    "    if lipids!= []:\n",
    "        for i in range(0,len(lipids)):\n",
    "            membrane = membrane + lipids[i]\n",
    "        print(\"membrane\") \n",
    "        print(membrane)  \n",
    "        R_membrane_z = membrane.center_of_mass()[2]\n",
    "    print(R_membrane_z)\n",
    "    \n",
    "#####number of each lipid per leaflet\n",
    "        \n",
    "    for key_mol in lipids_dict:\n",
    "        leaflet1 = 0 \n",
    "        leaflet2 = 0 \n",
    "        \n",
    "        selection = \"\"\n",
    "        if key_mol in sim['MAPPING_DICT'].keys():\n",
    "            m_file = sim['MAPPING_DICT'][key_mol]\n",
    "            with open('./mapping_files/'+m_file,\"r\") as f:\n",
    "                    for line in f:\n",
    "                        if len(line.split()) > 2 and \"Individual atoms\" not in line:\n",
    "                            selection = selection + \"resname \" + line.split()[2] + \" and name \" + line.split()[1] + \" or \"\n",
    "                        elif \"Individual atoms\" in line:\n",
    "                            continue\n",
    "                        else:\n",
    "                            selection = \"resname \" + sim.get(key_mol)\n",
    "                            break\n",
    "        selection = selection.rstrip(' or ')\n",
    "   #     print(selection)\n",
    "        molecules = u.select_atoms(selection)\n",
    "        print(molecules.residues)\n",
    "        x = 'N' + key_mol\n",
    "        if molecules.n_residues > 0:\n",
    "            for mol in molecules.residues:\n",
    "                R = mol.atoms.center_of_mass()\n",
    "                \n",
    "                if R[2] - R_membrane_z > 0:\n",
    "                    leaflet1 = leaflet1 + 1\n",
    "                       # print('layer1  ' + str(leaflet1))\n",
    "                elif R[2] - R_membrane_z < 0:\n",
    "                    leaflet2 = leaflet2 +1\n",
    "                       # print('layer2  ' + str(leaflet2))\n",
    "        sim[x] = [leaflet1, leaflet2] \n",
    "\n",
    "        \n",
    "#print(\"upper leaflet: \" + str(leaflet1))\n",
    "#print(\"lower leaflet: \" + str(leaflet2))\n",
    "\n",
    "###########################################################################################        \n",
    "#numbers of other molecules\n",
    "    for key_mol in molecules_dict:\n",
    "        value_mol = sim.get(key_mol)\n",
    "        print(value_mol)\n",
    "        x = 'N' + key_mol\n",
    "        sim[x] = u.select_atoms(\"resname \" + value_mol ).n_residues\n",
    "        \n",
    "        \n",
    "\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        :-) GROMACS - gmx dump, 2020 (-:\n",
      "\n",
      "                            GROMACS is written by:\n",
      "     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen\n",
      "    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    \n",
      " Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     \n",
      "  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  \n",
      "  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis\n",
      "    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    \n",
      "  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   \n",
      "    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   \n",
      "    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  \n",
      "   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen \n",
      " Christian Wennberg    Maarten Wolf      Artem Zhmurov   \n",
      "                           and the project leaders:\n",
      "        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel\n",
      "\n",
      "Copyright (c) 1991-2000, University of Groningen, The Netherlands.\n",
      "Copyright (c) 2001-2019, The GROMACS development team at\n",
      "Uppsala University, Stockholm University and\n",
      "the Royal Institute of Technology, Sweden.\n",
      "check out http://www.gromacs.org for more information.\n",
      "\n",
      "GROMACS is free software; you can redistribute it and/or modify it\n",
      "under the terms of the GNU Lesser General Public License\n",
      "as published by the Free Software Foundation; either version 2.1\n",
      "of the License, or (at your option) any later version.\n",
      "\n",
      "GROMACS:      gmx dump, version 2020\n",
      "Executable:   /usr/local/gromacs/bin/gmx\n",
      "Data prefix:  /usr/local/gromacs\n",
      "Working dir:  /media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts\n",
      "Command line:\n",
      "  gmx dump -s /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/md_0.tpr\n",
      "\n",
      "Reading file /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/md_0.tpr, VERSION 2018.3 (single precision)\n",
      "Reading file /media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/md_0.tpr, VERSION 2018.3 (single precision)\n",
      "Note: file tpx version 112, software tpx version 119\n",
      "\n",
      "GROMACS reminds you: \"If at first you don't succeed, try two more times so that your failure is statistically significant.\" (Dallas Warren)\n",
      "\n",
      "{'ID': 0, 'DOI': '10.5281/zenodo.3266240', 'MAPPING': 'POPC,mappingPOPCgromos-ckp.txt,POPG,mappingPOPGgromos-ckp.txt', 'SYSTEM': 'POPCPOPG_7:3_T310K', 'SOFTWARE': 'gromacs', 'FF': 'GROMOS-CKP', 'FF_SOURCE': '??', 'FF_DATE': '?/?/????', 'TRJ': [['total_4_500.xtc']], 'TPR': [['md_0.tpr']], 'PREEQTIME': '0', 'TIMELEFTOUT': '400', 'UNITEDATOM': 'POPC,GROMOS_CKP_POPC,POPG,GROMOS_CKP_POPG', 'POPC': 'POPC', 'POPG': 'LPOG', 'POPS': 'POPS', 'POPE': 'POPE', 'POT': 'K', 'SOD': 'NA', 'CLA': 'CL', 'CAL': 'CA', 'SOL': 'SOL', 'NPOPC': [250, 250], 'NPOPG': [0, 0], 'NPOPS': [0, 0], 'NPOPE': [0, 0], 'NPOT': 0, 'NSOD': 0, 'NCLA': 0, 'NCAL': 0, 'NSOL': 25000, 'TEMPERATURE': '310', 'TRJLENGTH': 100010.0, 'MAPPING_DICT': {'POPC': 'mappingPOPCgromos-ckp.txt', 'POPG': 'mappingPOPGgromos-ckp.txt'}, 'UADICTIONARY': {'POPC': 'GROMOS_CKP_POPC', 'POPG': 'GROMOS_CKP_POPG'}, 'GRO': '/media/akiirikk/DATADRIVE1/tietokanta/Data/tmp/DATABANK/tmp/0/conf.gro'}\n"
     ]
    }
   ],
   "source": [
    "#Anne: Read temperature and trajectory length from tpr file\n",
    "import re\n",
    "\n",
    "dt = 0\n",
    "nsteps = 0\n",
    "nstxout = 0\n",
    "\n",
    "for sim in sims_working_links:\n",
    "    ID = sim.get('ID')\n",
    "    tpr = str(sim.get('TPR')).translate({ord(c): None for c in \"'][\"})\n",
    "    tpr_path = str(dir_wrk) + '/tmp/' + str(ID) + '/' + tpr\n",
    "    trj = str(sim.get('TRJ')).translate({ord(c): None for c in \"'][\"})\n",
    "    trj_path = str(dir_wrk) + '/tmp/' + str(ID) + '/' + trj\n",
    "    \n",
    "    !echo System | gmx dump -s {tpr_path} > tpr.txt\n",
    "    \n",
    "    file1 = str(dir_wrk) + '/tmp/' + str(ID) + '/tpr.txt'\n",
    "\n",
    "    with open(\"tpr.txt\", 'rt') as tpr_info:\n",
    "        for line in tpr_info:\n",
    "            if 'ref-t' in line:\n",
    "                sim['TEMPERATURE']=line.split()[1]\n",
    "    \n",
    "    mol = Universe(tpr_path, trj_path)\n",
    "    Nframes=len(mol.trajectory)\n",
    "    timestep = mol.trajectory.dt\n",
    " \n",
    "    trj_length = Nframes * timestep\n",
    "   \n",
    "    sim['TRJLENGTH'] = trj_length\n",
    "    \n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to databank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../Data/Simulations/35d’: File exists\n",
      "mkdir: cannot create directory ‘../Data/Simulations/35d/d6b’: File exists\n",
      "mkdir: cannot create directory ‘../Data/Simulations/35d/d6b/35dd6bcb5bf03400b81c070292e36025c48dc1a6’: File exists\n",
      "mkdir: cannot create directory ‘../Data/Simulations/35d/d6b/35dd6bcb5bf03400b81c070292e36025c48dc1a6/3d0ca721c24d7f22d09178f10f1dd89a333dfe07’: File exists\n"
     ]
    }
   ],
   "source": [
    "#import json\n",
    "import yaml\n",
    "\n",
    "data_directory = {}\n",
    "for sim in sims_working_links:\n",
    "    ID=sim.get('ID')\n",
    "    # Batuhan: Creating a nested directory structure as discussed on the Issue here https://github.com/NMRLipids/NMRlipidsVIpolarizableFFs/issues/3\n",
    "    \n",
    "    head_dir = sims_hashes[ID].get('TPR')[0][1][0:3]\n",
    "    sub_dir1 = sims_hashes[ID].get('TPR')[0][1][3:6]\n",
    "    sub_dir2 = sims_hashes[ID].get('TPR')[0][1]\n",
    "    sub_dir3 = sims_hashes[ID].get('TRJ')[0][1]\n",
    "    \n",
    "    !mkdir {'../Data/Simulations/' + str(head_dir)}\n",
    "    !mkdir {'../Data/Simulations/' + str(head_dir) + '/' + str(sub_dir1)}\n",
    "    !mkdir {'../Data/Simulations/' + str(head_dir) + '/' + str(sub_dir1) + '/' + str(sub_dir2)}\n",
    "    !mkdir {'../Data/Simulations/' + str(head_dir) + '/' + str(sub_dir1) + '/' + str(sub_dir2) + '/' + str(sub_dir3)}\n",
    "    \n",
    "    DATAdir = '../Data/Simulations/' + str(head_dir) + '/' + str(sub_dir1) + '/' + str(sub_dir2) + '/' + str(sub_dir3)\n",
    "    data_directory[str(ID)] = DATAdir\n",
    "    \n",
    "    # SAMULI: I am writin now in txt, but using json might be better in the future\n",
    "   # outfileDICT=open(str(DATAdir)+'/README.md','w')\n",
    "   # outfileDICT=str(dir_wrk)+'/tmp/'+str(ID)+'/'+ 'README.json'\n",
    "    \n",
    "   # with open(outfileDICT, 'w') as f:\n",
    "   #     json.dump(sim, f)\n",
    "   \n",
    "\n",
    "    #!cp {str(dir_wrk)}'/tmp/'{str(ID)}'/README.json' {data_directory.get(str(ID))}  \n",
    "# dictionary saved in yaml format\n",
    "    outfileDICT=str(dir_wrk)+'/tmp/'+str(ID)+'/'+ 'README.yaml'\n",
    "    \n",
    "    with open(outfileDICT, 'w') as f:\n",
    "        yaml.dump(sim,f, sort_keys=False)\n",
    "        \n",
    "    !cp {str(dir_wrk)}'/tmp/'{str(ID)}'/README.yaml' {data_directory.get(str(ID))}\n",
    "    #outfileDICT.write(str(sim))\n",
    "    #outfileDICT.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis starts here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating order parameters\n",
      "M_G1H1_M POPC C32 H321\n",
      "\n",
      "M_G1H2_M POPC C32 H322\n",
      "\n",
      "M_G1C3H1_M POPC C36 H361\n",
      "\n",
      "M_G1C3H2_M POPC C36 H362\n",
      "\n",
      "M_G1C4H1_M POPC C37 H371\n",
      "\n",
      "M_G1C4H2_M POPC C37 H372\n",
      "\n",
      "M_G1C5H1_M POPC C38 H381\n",
      "\n",
      "M_G1C5H2_M POPC C38 H382\n",
      "\n",
      "M_G1C6H1_M POPC C39 H391\n",
      "\n",
      "M_G1C6H2_M POPC C39 H392\n",
      "\n",
      "M_G1C7H1_M POPC C40 H401\n",
      "\n",
      "M_G1C7H2_M POPC C40 H402\n",
      "\n",
      "M_G1C8H1_M POPC C41 H411\n",
      "\n",
      "M_G1C8H2_M POPC C41 H412\n",
      "\n",
      "M_G1C9H1_M POPC C42 H421\n",
      "\n",
      "M_G1C9H2_M POPC C42 H422\n",
      "\n",
      "M_G1C10H1_M POPC C43 H431\n",
      "\n",
      "M_G1C10H2_M POPC C43 H432\n",
      "\n",
      "M_G1C11H1_M POPC C44 H441\n",
      "\n",
      "M_G1C11H2_M POPC C44 H442\n",
      "\n",
      "M_G1C12H1_M POPC C45 H451\n",
      "\n",
      "M_G1C12H2_M POPC C45 H452\n",
      "\n",
      "M_G1C13H1_M POPC C46 H461\n",
      "\n",
      "M_G1C13H2_M POPC C46 H462\n",
      "\n",
      "M_G1C14H1_M POPC C47 H471\n",
      "\n",
      "M_G1C14H2_M POPC C47 H472\n",
      "\n",
      "M_G1C15H1_M POPC C48 H481\n",
      "\n",
      "M_G1C15H2_M POPC C48 H482\n",
      "\n",
      "M_G1C16H1_M POPC C49 H491\n",
      "\n",
      "M_G1C16H2_M POPC C49 H492\n",
      "\n",
      "M_G1C17H1_M POPC C50 H501\n",
      "\n",
      "M_G1C17H2_M POPC C50 H502\n",
      "\n",
      "M_G1C17H2_M POPC C50 H503\n",
      "\n",
      "M_G2H1_M POPC C13 H131\n",
      "\n",
      "M_G2C3H1_M POPC C17 H171\n",
      "\n",
      "M_G2C3H2_M POPC C17 H172\n",
      "\n",
      "M_G2C4H1_M POPC C18 H181\n",
      "\n",
      "M_G2C4H2_M POPC C18 H182\n",
      "\n",
      "M_G2C5H1_M POPC C19 H191\n",
      "\n",
      "M_G2C5H2_M POPC C19 H192\n",
      "\n",
      "M_G2C6H1_M POPC C20 H201\n",
      "\n",
      "M_G2C6H2_M POPC C20 H202\n",
      "\n",
      "M_G2C7H1_M POPC C21 H211\n",
      "\n",
      "M_G2C7H2_M POPC C21 H212\n",
      "\n",
      "M_G2C8H1_M POPC C22 H221\n",
      "\n",
      "M_G2C8H2_M POPC C22 H222\n",
      "\n",
      "M_G2C9H1_M POPC C23 H231\n",
      "\n",
      "M_G2C9H2_M POPC C23 H232\n",
      "\n",
      "M_G2C10H1_M POPC C24 H24\n",
      "\n",
      "M_G2C11H1_M POPC C25 H25\n",
      "\n",
      "M_G2C12H1_M POPC C26 H261\n",
      "\n",
      "M_G2C12H2_M POPC C26 H262\n",
      "\n",
      "M_G2C13H1_M POPC C27 H271\n",
      "\n",
      "M_G2C13H2_M POPC C27 H272\n",
      "\n",
      "M_G2C14H1_M POPC C28 H281\n",
      "\n",
      "M_G2C14H2_M POPC C28 H282\n",
      "\n",
      "M_G2C15H1_M POPC C29 H291\n",
      "\n",
      "M_G2C15H2_M POPC C29 H292\n",
      "\n",
      "M_G2C16H1_M POPC C30 H301\n",
      "\n",
      "M_G2C16H2_M POPC C30 H302\n",
      "\n",
      "M_G2C17H1_M POPC C31 H311\n",
      "\n",
      "M_G2C17H2_M POPC C31 H312\n",
      "\n",
      "M_G2C18H1_M POPC CA1 H511\n",
      "\n",
      "M_G2C18H2_M POPC CA1 H512\n",
      "\n",
      "M_G2C19H1_M POPC CA2 H521\n",
      "\n",
      "M_G2C19H2_M POPC CA2 H522\n",
      "\n",
      "M_G2C19H3_M POPC CA2 H523\n",
      "\n",
      "M_G3H1_M POPC C12 H121\n",
      "\n",
      "M_G3H2_M POPC C12 H122\n",
      "\n",
      "M_G3C4H1_M POPC C6 H61\n",
      "\n",
      "M_G3C4H2_M POPC C6 H62\n",
      "\n",
      "M_G3C5H1_M POPC C5 H51\n",
      "\n",
      "M_G3C5H2_M POPC C5 H52\n",
      "\n",
      "M_G3N6C1H1_M POPC C1 H11\n",
      "\n",
      "M_G3N6C1H2_M POPC C1 H12\n",
      "\n",
      "M_G3N6C1H3_M POPC C1 H13\n",
      "\n",
      "M_G3N6C2H1_M POPC C2 H21\n",
      "\n",
      "M_G3N6C2H2_M POPC C2 H22\n",
      "\n",
      "M_G3N6C2H3_M POPC C2 H23\n",
      "\n",
      "M_G3N6C3H1_M POPC C3 H31\n",
      "\n",
      "M_G3N6C3H2_M POPC C3 H32\n",
      "\n",
      "M_G3N6C3H3_M POPC C3 H33\n",
      "\n",
      "Constructing the system...\n",
      "System has 101000 atoms\n",
      "Dealing with frame 0 at 400000.0 ps.\n",
      "Dealing with frame 1 at 400010.0 ps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f5f8bb98f240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mdeffile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_wrk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/tmp/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.def'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mlipidname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UADICTIONARY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mbuildH_calcOP_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlipidname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeffile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtcwhole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mordPfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;31m#print(buildH_calcOP.make_dic_atname2genericname(def_file))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(topfile, lipid, def_file, trajfile, outOP, opdbxtc)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;31m# is modified in place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mfast_build_all_Hs_calc_OP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniverse_woH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic_OP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic_lipid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic_Cname2Hnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;31m# 7) Output results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mfast_build_all_Hs_calc_OP\u001b[0;34m(universe_woH, dic_OP, dic_lipid, dic_Cname2Hnames)\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0;31m# Get newly built H(s) on that atom.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 Hs_coor = fast_buildHs_on_1C(dic_lipids_with_indexes, ts,\n\u001b[0;32m--> 967\u001b[0;31m                                              Cname, ix_first_atom_res)\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cname_position with fast indexing:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCname_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mfast_buildHs_on_1C\u001b[0;34m(dic_lipids_with_indexes, ts, Cname, ix_first_atom_res)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Build new H(s) and get coordinates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtypeofH2build\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CH2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mH1_coor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH2_coor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_CH2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCname_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper1_coor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper2_coor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH1_coor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH2_coor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtypeofH2build\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CH\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mget_CH2\u001b[0;34m(atom, helper1, helper2)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mhcoor_H1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLENGTH_CH_BOND\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_vect_H1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     unit_vect_H2 = apply_rotation(vec_to_rotate, rotation_axis,\n\u001b[0;32m--> 331\u001b[0;31m                                  TETRAHEDRAL_ANGLE/2)\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0mhcoor_H2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLENGTH_CH_BOND\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_vect_H2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhcoor_H1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhcoor_H2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mapply_rotation\u001b[0;34m(vec_to_rotate, rotation_axis, rad_angle)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# Generate a quaternion of the given angle (in radian).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mquaternion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec2quaternion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrad_angle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Generate the rotation matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mrotation_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_rotation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquaternion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mvec2quaternion\u001b[0;34m(vec, theta)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[1;32m    167\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/akiirikk/DATADRIVE1/tietokanta/NMRlipidsIVPEandPG/scripts/DataBankINFO/buildH_calcOP_test.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \"\"\"Normalizes a vector.\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from OrderParameter import *\n",
    "import warnings\n",
    "#from corrtimes import *\n",
    "import subprocess\n",
    "import mdtraj\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, './DataBankINFO')\n",
    "import buildH_calcOP_test\n",
    "\n",
    "#!cp corr_ftios_ind.sh {dir_wrk}\n",
    "for sim in sims_working_links:\n",
    "    trj=sim.get('TRJ')\n",
    "    tpr=sim.get('TPR')\n",
    "    ID=sim.get('ID')\n",
    "    software=sim.get('SOFTWARE')\n",
    "    preEQ=sim.get('PREEQTIME')\n",
    "    unitedAtom=sim.get('UNITEDATOM')\n",
    "    \n",
    "    ext=trj[0:-3] # getting the trajectory extension\n",
    "    \n",
    "    # BATUHAN: Adding a few lines to convert the trajectory into .xtc using MDTRAJ\n",
    "    #          We will need users to install MDTRAJ in their system so that we can convert other trajectories into xtc\n",
    "\n",
    "    if software != \"gromacs\":\n",
    "        \n",
    "        print(\"converting the trajectory into xtc\")\n",
    "        \n",
    "        pdb = sim.get('PDB')\n",
    "        output_traj = str(dir_wrk) + '/tmp/' + str(ID) + '/' + 'tmp_converted.xtc'\n",
    "        input_traj = str(dir_wrk) + '/tmp/' + str(ID) + '/' + trj[0][0]\n",
    "        input_pdb = str(dir_wrk) + '/tmp/' + str(ID) + '/' + pdb[0][0]\n",
    "      \n",
    "        if os.path.isfile(output_traj): # when we're done with the converted trajectory we can simply remove it\n",
    "            !rm {output_traj}\n",
    "        \n",
    "        !echo System | mdconvert {input_traj} -o {output_traj} -t {input_pdb} --force # force overwrite\n",
    "        \n",
    "        # SAMULI: this xtcwhole does not necessarily have molecules as whole. Only if {input_traj} has.\n",
    "        xtcwhole = str(dir_wrk) + '/tmp/' + str(ID) + '/' + 'tmp_converted.xtc'\n",
    "        tpr=input_pdb\n",
    "        \n",
    "        print(\"trajectory conversion is completed\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        xtc = str(dir_wrk) + '/tmp/' + str(ID) + '/' + str(trj[0][0])  \n",
    "        tpr = str(dir_wrk) + '/tmp/' + str(ID) + '/' + str(tpr[0][0])\n",
    "        xtcwhole=str(dir_wrk)+'/tmp/'+str(ID)+'/whole.xtc'\n",
    "   #     !echo System | gmx trjconv -f {xtc} -s {tpr} -o {xtcwhole} -pbc mol -b {preEQ}\n",
    "   \n",
    "\n",
    "    \n",
    "    print(\"Calculating order parameters\") \n",
    "    \n",
    "    if unitedAtom:\n",
    "        for key in sim['UADICTIONARY']:\n",
    "            def_file = open(str(dir_wrk) + '/tmp/' + str(ID) + '/' + key + '.def', 'w')\n",
    "\n",
    "            mapping_file = sim['MAPPING_DICT'][key]\n",
    "            previous_line = \"\"\n",
    "            \n",
    "            with open('mapping_files/'+mapping_file, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    if not line.startswith(\"#\"):\n",
    "                        regexp1_H = re.compile(r'M_[A-Z0-9]*C[0-9]*H[0-9]*_M')\n",
    "                        regexp2_H = re.compile(r'M_G[0-9]*H[0-9]*_M')\n",
    "                        regexp1_C = re.compile(r'M_[A-Z0-9]*C[0-9]*_M')\n",
    "                        regexp2_C = re.compile(r'M_G[0-9]_M')\n",
    "\n",
    "                        if regexp1_C.search(line) or regexp2_C.search(line):\n",
    "                            atomC = line.split()\n",
    "                            atomH = []\n",
    "                        elif regexp1_H.search(line) or regexp2_H.search(line):\n",
    "                            atomH = line.split()\n",
    "                        else:\n",
    "                            atomC = []\n",
    "                            atomH = []\n",
    "\n",
    "                        if atomH:\n",
    "                            items = [atomC[1], atomH[1], atomC[0], atomH[0]]\n",
    "                            def_line = items[3] + \" \" + key + \" \" + items[0] + \" \" + items[1] + \"\\n\"\n",
    "                            if def_line != previous_line:\n",
    "                                def_file.write(def_line)\n",
    "                                print(def_line)\n",
    "                                previous_line = def_line\n",
    "            def_file.close()\n",
    "        #Add hydrogens with buildH\n",
    "            ordPfile = str(dir_wrk) + '/tmp/' + str(ID) + '/' + key + '_order_parameter.dat'\n",
    "            topfile = sim.get('GRO')\n",
    "            deffile = str(dir_wrk) + '/tmp/' + str(ID) + '/' + key + '.def'\n",
    "            lipidname = sim['UADICTIONARY'][key]\n",
    "            buildH_calcOP_test.main(topfile,lipidname,deffile,xtcwhole,ordPfile)\n",
    "            #print(buildH_calcOP.make_dic_atname2genericname(def_file))\n",
    "    else:\n",
    "        for key in sim['MAPPING_DICT']:\n",
    "            outfile=open(str(dir_wrk)+'/tmp/'+str(ID)+'/'+key+'OrderParameters.dat','w')\n",
    "            line1=\"Atom     Average OP     OP stem\"+'\\n'\n",
    "            outfile.write(line1)\n",
    "    \n",
    "            data = {}\n",
    "            outfile2=str(dir_wrk)+'/tmp/'+str(ID)+'/'+key+'OrderParameters.json'\n",
    "        \n",
    "            mapping_file = sim['MAPPING_DICT'][key]\n",
    "            print(key)\n",
    "    \n",
    "     \n",
    "            OrdParam=find_OP('mapping_files/'+mapping_file,tpr,xtcwhole,key)\n",
    "      \n",
    "        for i,op in enumerate(OrdParam):\n",
    "            resops =op.get_op_res\n",
    "            (op.avg, op.std, op.stem) =op.get_avg_std_stem_OP\n",
    "            line2=str(op.name)+\" \"+str(op.avg)+\" \"+str(op.stem)+'\\n'\n",
    "            outfile.write(line2)\n",
    "    \n",
    "            data[str(op.name)]=[]\n",
    "            data[str(op.name)].append(op.get_avg_std_stem_OP)\n",
    "        \n",
    "        with open(outfile2, 'w') as f:\n",
    "            json.dump(data,f)\n",
    "\n",
    "        outfile.close()\n",
    "        !cp {str(dir_wrk)}'/tmp/'{str(ID)}'/'{key}'OrderParameters.dat' {data_directory.get(str(ID))}    \n",
    "        !cp {str(dir_wrk)}'/tmp/'{str(ID)}'/'{key}'OrderParameters.json' {data_directory.get(str(ID))}  \n",
    "    \n",
    "    print(\"Done calculating order parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
